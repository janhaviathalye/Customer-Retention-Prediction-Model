{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-jn9tcCULb3"
   },
   "outputs": [],
   "source": [
    "#Customer Retention Prediction Model - AMEX Shadowing Program\n",
    "\n",
    "\n",
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import BaggingClassifier # Bagging Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier # Random Forest Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.linear_model import LogisticRegression # Import Logistic Regression Classifier\n",
    "from sklearn.svm import SVC # Import SVM classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, make_scorer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "\n",
    "!pip install xgboost\n",
    "!pip install imbalanced-learn\n",
    "!pip install lightgbm catboost imbalanced-learn\n",
    "\n",
    "!source myenv/bin/activate  # Unix/MacOS\n",
    "\n",
    "!pip install numpy pandas scikit-learn xgboost lightgbm catboost imbalanced-learn\n",
    "\n",
    "!pip install papermill\n",
    "\n",
    "!pip install flask nbformat papermill lightgbm imbalanced-learn scikit-learn\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing and modeling\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Suppress warnings (optional)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -------------------------\n",
    "# Parameters\n",
    "test_data_path = 'Churn_Modelling_Test.csv'  # Default value, overridden by papermill\n",
    "metrics_output_path = 'metrics_output.json'  # Default value, overridden by papermill\n",
    "\n",
    "# Feature Engineering Function\n",
    "def add_features(data):\n",
    "    data['BalanceSalaryRatio'] = data['Balance'] / (data['EstimatedSalary'] + 1)\n",
    "    data['CreditScoreAgeRatio'] = data['CreditScore'] / (data['Age'] + 1)\n",
    "    data['TenureByAge'] = data['Tenure'] / (data['Age'] + 1)\n",
    "    data['AgeBalanceInteraction'] = data['Age'] * data['Balance']\n",
    "    data['HasCrCardAndActive'] = data['HasCrCard'] * data['IsActiveMember']\n",
    "    return data\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('Churn_Modelling_Train.csv')\n",
    "test_data = pd.read_csv(test_data_path)  # Use the parameter for test data path\n",
    "\n",
    "# Add features\n",
    "train_data = add_features(train_data)\n",
    "test_data = add_features(test_data)\n",
    "\n",
    "# Preprocess data function\n",
    "\n",
    "# def preprocess_data(data):\n",
    "#     # Keep CustomerId and Surname separately\n",
    "#     # customer_info = data[['CustomerId', 'Surname']].copy()\n",
    "    \n",
    "#     # Drop unnecessary columns\n",
    "#     data = data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1, errors='ignore')\n",
    "    \n",
    "#     X = data.drop('Exited', axis=1)\n",
    "#     y = data['Exited']\n",
    "    \n",
    "#     return X, y\n",
    "\n",
    "# def preprocess_data(data):\n",
    "#     # Keep CustomerId and Surname separately\n",
    "#     customer_info = data[['CustomerId', 'Surname']].copy()\n",
    "    \n",
    "#     # Drop unnecessary columns\n",
    "#     data = data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1, errors='ignore')\n",
    "    \n",
    "#     X = data.drop('Exited', axis=1)\n",
    "#     y = data['Exited']\n",
    "    \n",
    "#     return X, y, customer_info\n",
    "\n",
    "# # X_train_raw, y_train_raw = preprocess_data(train_data)\n",
    "# # X_test_raw, y_test = preprocess_data(test_data)\n",
    "\n",
    "# # Preprocess data\n",
    "# X_train_raw, y_train_raw, _ = preprocess_data(train_data)  # The underscore (_) ignores the third returned value for training data\n",
    "# X_test_raw, y_test, test_customers = preprocess_data(test_data)  # Assign the third returned value to test_customers\n",
    "\n",
    "# Preprocess data function\n",
    "def preprocess_data(data):\n",
    "    # Keep CustomerId and Surname separately\n",
    "    customer_info = data[['CustomerId', 'Surname']].copy()\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    data = data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1, errors='ignore')\n",
    "    \n",
    "    X = data.drop('Exited', axis=1)\n",
    "    y = data['Exited']\n",
    "    \n",
    "    return X, y, customer_info\n",
    "\n",
    "# Preprocess data\n",
    "X_train_raw, y_train_raw, _ = preprocess_data(train_data)  # Ignore customer_info for training data\n",
    "X_test_raw, y_test, test_customers = preprocess_data(test_data)  # Capture customer_info for test data\n",
    "\n",
    "\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_cols = ['Geography', 'Gender']\n",
    "numerical_cols = [col for col in X_train_raw.columns if col not in categorical_cols]\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = StandardScaler()\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = OneHotEncoder(drop='first', handle_unknown='ignore')\n",
    "\n",
    "# Bundle preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define the model (LightGBM)\n",
    "lgb_classifier = lgb.LGBMClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Simplified hyperparameter grid\n",
    "param_dist = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__learning_rate': [0.05, 0.1],\n",
    "    'classifier__max_depth': [3, 5, 7],\n",
    "    'classifier__num_leaves': [31, 50],\n",
    "    'classifier__min_child_samples': [20, 30],\n",
    "    'classifier__subsample': [0.8, 1.0],\n",
    "    'classifier__colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Custom scorer\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# Pipeline\n",
    "pipeline = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42, k_neighbors=5)),\n",
    "    ('classifier', lgb_classifier)\n",
    "])\n",
    "\n",
    "# Cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Randomized Search\n",
    "n_iter_search = 20  # Number of parameter settings that are sampled\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=n_iter_search,\n",
    "    cv=cv,\n",
    "    scoring=f1_scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train_raw, y_train_raw)\n",
    "\n",
    "# Best estimator\n",
    "best_model = random_search.best_estimator_\n",
    "print(f\"Best parameters for LightGBM: {random_search.best_params_}\")\n",
    "print(f\"Best cross-validated F1 score: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# Transform the test data using the fitted preprocessor from the pipeline\n",
    "X_test_transformed = best_model.named_steps['preprocessor'].transform(X_test_raw)\n",
    "\n",
    "# Predict probabilities on test data\n",
    "y_probs = best_model.named_steps['classifier'].predict_proba(X_test_transformed)[:, 1]\n",
    "\n",
    "# Optimize threshold\n",
    "thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "f1_scores = [f1_score(y_test, (y_probs >= t).astype(int)) for t in thresholds]\n",
    "optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "max_f1_score = max(f1_scores)\n",
    "\n",
    "print(f\"\\nOptimal Threshold: {optimal_threshold:.2f}\")\n",
    "print(f\"Max F1 Score on Test Data: {max_f1_score:.4f}\")\n",
    "\n",
    "# Final predictions\n",
    "y_pred_optimal = (y_probs >= optimal_threshold).astype(int)\n",
    "\n",
    "test_customers['Exited_Predicted'] = y_pred_optimal\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_test = accuracy_score(y_test, y_pred_optimal)\n",
    "precision_test = precision_score(y_test, y_pred_optimal)\n",
    "recall_test = recall_score(y_test, y_pred_optimal)\n",
    "f1_test = f1_score(y_test, y_pred_optimal)\n",
    "\n",
    "print(f\"\\nOptimized Model Performance on Test Data:\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"F1_score: {f1_test:.4f}\")\n",
    "\n",
    "# Save the metrics to a JSON file\n",
    "metrics = {\n",
    "    'Accuracy': accuracy_test,\n",
    "    'Precision': precision_test,\n",
    "    'Recall': recall_test,\n",
    "    'F1_score': f1_test\n",
    "}\n",
    "\n",
    "import json\n",
    "\n",
    "with open(metrics_output_path, 'w') as f:\n",
    "    json.dump(metrics, f)\n",
    "\n",
    "\n",
    "# Filter customers who exited\n",
    "exited_customers = test_customers[test_customers['Exited_Predicted'] == 1][['CustomerId', 'Surname']]\n",
    "exited_customers.to_json('exited_customers.json', orient = 'records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clKZfzEFUAgo"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
